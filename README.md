# Assistant-Teacher: серверная часть агентной системы поддержки преподавателя

## Содержание

1. [Обзор и назначение](#1-обзор-и-назначение)
    1. [Цель проекта](#11-цель-проекта)
    2. [Место в общем проекте](#12-место-в-общем-проекте)
    3. [Краткое описание архитектуры](#13-краткое-описание-архитектуры)
2. [Структура репозитория и компоненты](#2-структура-репозитория-и-компоненты)
    1. [Сервисы](#21-сервисы)
    2. [Инфраструктурные файлы](#22-инфраструктурные-файлы)
    3. [Сценарии и вспомогательные утилиты](#23-сценарии-и-вспомогательные-утилиты)
3. [Зависимости и требования](#3-зависимости-и-требования)
    1. [Программные зависимости](#31-программные-зависимости)
    2. [Аппаратные требования](#32-аппаратные-требования)
4. [Установка, конфигурация и запуск](#4-установка-конфигурация-и-запуск)
    1. [Подготовка окружения](#41-подготовка-окружения)
    2. [Конфигурация `.env`](#42-конфигурация-env)
    3. [Сборка и запуск Docker-стека](#43-сборка-и-запуск-docker-стека)
5. [Описание логики и функциональности модулей](#5-описание-логики-и-функциональности-модулей)
    1. [API-сервис (`services/api`)](#51-api-сервис-servicesapi)
    2. [Сервис эмбеддингов Vectorizer (`services/vectorizer`)](#52-сервис-эмбеддингов-vectorizer-servicesvectorizer)
    3. [Фоновые воркеры (`services/worker`)](#53-фоновые-воркеры-servicesworker)
    4. [Индексирование PDF (`ingest_pdfs.py`)](#54-индексирование-pdf-ingest_pdfspy)
6. [Примеры использования](#6-примеры-использования)
    1. [Ответы на вопросы (RAG)](#61-ответы-на-вопросы-rag)
    2. [Универсальный ассистент](#62-универсальный-ассистент)
    3. [Суммаризация лекций](#63-суммаризация-лекций)
    4. [Генерация квизов](#64-генерация-квизов)
    5. [Проверка квизов](#65-проверка-квизов)
7. [Возможные ошибки и узкие места](#7-возможные-ошибки-и-узкие-места)
    1. [Инфраструктура и сеть](#71-инфраструктура-и-сеть)
    2. [Модели и ресурсы](#72-модели-и-ресурсы)
    3. [Качество данных и индексации](#73-качество-данных-и-индексации)
    4. [Логика LLM и формат ответов](#74-логика-llm-и-формат-ответов)
8. [FAQ (часто задаваемые вопросы)](#8-faq-часто-задаваемые-вопросы)
9. [Рекомендации и лучшие практики](#9-рекомендации-и-лучшие-практики)

---

## 1. Обзор и назначение

### 1.1. Цель проекта

**Assistant-Teacher** – это серверная система поддержки преподавателя, работающая поверх набора учебных материалов (в первую очередь PDF-лекций).  

Система предоставляет следующие функции:

- семантический и полнотекстовый поиск по материалам курса;
- ответы на вопросы студентов на основе загруженных материалов (RAG-подход);
- автоматическая генерация тестовых заданий (квизов);
- автоматическая проверка ответов студентов на квизы;
- суммаризация (краткие конспекты) лекций;
- перевод материалов с английского на русский и упрощение сложных текстов.

Все вычисления выполняются локально, без обязательного обращения к внешним облачным API.

### 1.2. Место в общем проекте

Данный репозиторий реализует **серверную (backend) часть** агентной системы Assistant-Teacher.  

В рамках общего проекта предполагается, что:

- фронтенд (веб-клиент, чат-бот и т.п.) обращается к HTTP API;
- серверная часть отвечает за хранение и обработку материалов, работу LLM, формирование ответов, квизов и оценок.

Таким образом, этот код является ядром логики и данных системы, поверх которого могут строиться разные пользовательские интерфейсы.

### 1.3. Краткое описание архитектуры

Архитектура построена по микросервисному принципу:

- **API-сервис (FastAPI)** – единая точка входа для клиента.
- **Vectorizer** – сервис, считающий эмбеддинги для текста.
- **Worker** – набор фоновых воркеров: парсер материалов, генератор квизов, оценщик квизов.
- **Elasticsearch** – хранилище текстовых фрагментов и эмбеддингов.
- **MinIO (S3)** – хранилище исходных PDF и результатов квизов.
- **Kafka + Zookeeper** – шина событий и задач для воркеров.
- **LLM-сервер (llama.cpp)** – локальная языковая модель (Qwen 2.5 в формате GGUF).

Данные движутся по пайплайну:  
PDF → MinIO → Kafka → parser_worker → Elasticsearch + эмбеддинги → API → LLM → ответы/квизы/оценки.

---

## 2. Структура репозитория и компоненты

### 2.1. Сервисы

```text
services/
  api/         # HTTP API-сервис (FastAPI)
  vectorizer/  # сервис эмбеддингов
  worker/      # набор фоновых воркеров
````

* `services/api`
  Реализует REST-эндпоинты: ответы на вопросы, ассистент-роутер, суммаризация, квизы, проверка, перевод и переформулировка.

* `services/vectorizer`
  Принимает массив строк и возвращает эмбеддинги (массивы чисел), используется API и воркерами.

* `services/worker`
  Содержит три воркера:

  * `parser_worker.py` – разбор PDF и индексирование в Elasticsearch;
  * `generator_worker.py` – генерация квизов по курсу с помощью LLM;
  * `grader_worker.py` – проверка ответов на квизы и запись результата.

### 2.2. Инфраструктурные файлы

```text
docker-compose.yml  # конфигурация Docker-стека
.env                 # переменные окружения
infra/
  chunks.json        # схема индекса chunks в Elasticsearch
  es-init.sh         # пример инициализации индексов ES
  init.sql           # скрипт для PostgreSQL (опциональная часть проекта)
```

### 2.3. Сценарии и вспомогательные утилиты

* `ingest_pdfs.py`
  CLI-скрипт для оффлайн-индексирования PDF в Elasticsearch с помощью Vectorizer, без задействования MinIO и Kafka.

---

## 3. Зависимости и требования

### 3.1. Программные зависимости

Обязательные компоненты:

* Docker и Docker Compose (v2+);
* Python 3.10+ (для локального запуска отдельных частей при отладке);
* Elasticsearch (поднимается в Docker);
* MinIO (поднимается в Docker);
* Kafka + Zookeeper (поднимаются в Docker);
* llama.cpp server (контейнер с LLM);
* зависимости Python указаны в:

  * `services/api/requirements.txt`;
  * `services/vectorizer/requirements.txt` и `requirements.torch.txt`;
  * `services/worker/requirements.txt`.

### 3.2. Аппаратные требования

Минимальные ориентировочные требования:

* ОЗУ: от 8 ГБ (желательны 16+ для комфортной работы);
* CPU: 4+ ядра;
* Дисковое пространство:

  * модели LLM и эмбеддингов: 2–10 ГБ в зависимости от выбранной модели;
  * данные ES и MinIO зависят от объёма материалов;
* GPU (опционально): может использоваться для ускорения LLM и эмбеддингов, но система спроектирована так, чтобы работать и на CPU.

---

## 4. Установка, конфигурация и запуск

### 4.1. Подготовка окружения

1. Установить Docker и Docker Compose.
2. Склонировать репозиторий.
3. Подготовить каталог для моделей LLM:

```text
models/
  qwen2.5-1.5b-instruct-q4_k_m.gguf  # пример
```

4. Подготовить каталог для кэша моделей эмбеддингов:

```text
data/
  hf-cache/
```

(будет заполнен автоматически при первом запуске Vectorizer).

### 4.2. Конфигурация `.env`

Пример минимального `.env`:

```env
# учётные данные MinIO
MINIO_ROOT_USER=minioadmin
MINIO_ROOT_PASSWORD=minioadmin
```

При необходимости сюда добавляются дополнительные переменные для конкретных сервисов (см. README каждого сервиса).

### 4.3. Сборка и запуск Docker-стека

Сборка:

```bash
docker compose build
# или без кэша:
docker compose build --no-cache
```

Запуск:

```bash
docker compose up -d
```

Проверка:

```bash
curl http://localhost:8000/health      # API
curl http://localhost:8001/health      # Vectorizer
curl http://localhost:9200/            # Elasticsearch
curl http://localhost:8002/v1/models   # LLM-сервер
```

---

## 5. Описание логики и функциональности модулей

### 5.1. API-сервис (`services/api`)

Основные модули:

* `main.py` – регистрация роутов, endpoints для квизов, health-check и т.п.;
* `answers.py` – реализация RAG-ответов по материалам курса;
* `search.py` – текстовый и kNN-поиск по индексу `chunks`;
* `summary.py` – суммаризация лекций;
* `assistant.py` – универсальный ассистент, выбирающий режим работы через LLM;
* `routes_rephrase.py` / `rephrase_client.py` – переформулировка текста;
* `routes_translate_llm.py` / `translate.py` – перевод EN→RU и обработка команд типа «переведи на русский»;
* `settings.py` – конфигурация сервиса;
* `storage.py` – помощник для работы с MinIO;
* `security.py` – простые защиты от prompt-инъекций и утечки секретов.

Логика:

1. API принимает запрос от клиента.
2. Проверяет и очищает ввод (guard_input, сканирование секретов).
3. В зависимости от эндпоинта:

   * вызывает поиск по ES и LLM (RAG-ответ);
   * формирует задание в Kafka (генерация/проверка квиза);
   * обращается к Vectorizer (эмбеддинги);
   * возвращает структурированный JSON-ответ.

### 5.2. Сервис эмбеддингов Vectorizer (`services/vectorizer`)

* `app/main.py` – FastAPI-приложение с эндпоинтами:

  * `GET /health` – состояние;
  * `POST /embed` – получение эмбеддингов;
  * `POST /translate` – заглушка перевода.

Логика:

* При первом запросе загружает модель `SentenceTransformer` из кэша или скачивает;
* При `/embed`:

  * принимает список текстов;
  * возвращает список векторов (нормированных).

### 5.3. Фоновые воркеры (`services/worker`)

#### `parser_worker.py`

* Слушает Kafka-топик `materials.parse`.
* По каждому сообщению:

  * скачивает PDF из MinIO;
  * извлекает текст (PyMuPDF + OCR по картинкам);
  * разбивает на чанки (по длине и перекрытию);
  * запрашивает эмбеддинги у Vectorizer;
  * индексирует документы в Elasticsearch (индекс `chunks`).

#### `generator_worker.py`

* Слушает Kafka-топик `quizzes.generate`.
* По задаче:

  * делает поиск фрагментов по курсу в ES;
  * строит контекст;
  * вызывает LLM для генерации массива вопросов (JSON);
  * сохраняет квиз в MinIO;
  * публикует сообщение в `quizzes.generated`.

#### `grader_worker.py`

* Слушает Kafka-топик `quizzes.grade`.
* По задаче:

  * загружает квиз из MinIO;
  * сравнивает ответы студента с эталоном;
  * считает результат и подробности;
  * сохраняет в MinIO;
  * публикует сообщение в `quizzes.graded`.

### 5.4. Индексирование PDF (`ingest_pdfs.py`)

CLI-утилита, позволяющая:

* рекурсивно обойти каталог с PDF;
* извлечь текст со страниц (pypdf → PyMuPDF fallback);
* по желанию получить эмбеддинги через Vectorizer;
* в батчевом режиме проиндексировать всё в Elasticsearch.

Используется для быстрого наполнения индекса без MinIO/Kafka.

---

## 6. Примеры использования

### 6.1. Ответы на вопросы (RAG)

```bash
curl -X POST http://localhost:8000/answer \
  -H "Content-Type: application/json" \
  -d '{
    "course_id": "demo",
    "query": "Кратко объясни, что такое триггер засухи в этой модели",
    "k": 5,
    "num_candidates": 256
  }'
```

### 6.2. Универсальный ассистент

```bash
curl -X POST http://localhost:8000/assistant \
  -H "Content-Type: application/json" \
  -d '{
    "course_id": "demo",
    "query": "Сделай краткое содержание лекции по триггерам засухи",
    "k": 5,
    "num_candidates": 256,
    "quiz_n": 5
  }'
```

### 6.3. Суммаризация лекций

```bash
curl -X POST http://localhost:8000/summary/by_query \
  -H "Content-Type: application/json" \
  -d '{
    "course_id": "demo",
    "query": "лекция по триггерам засухи",
    "max_chars": 8000
  }'
```

### 6.4. Генерация квизов

```bash
curl -X POST http://localhost:8000/quizzes/generate \
  -H "Content-Type: application/json" \
  -d '{
    "course_id": "demo",
    "topics": ["триггеры засухи", "данные WorldPop"],
    "n": 5
  }'
```

### 6.5. Проверка квизов

```bash
curl -X POST http://localhost:8000/quizzes/grade \
  -H "Content-Type: application/json" \
  -d '{
    "job_id": "<quiz_job_id>",
    "answers": [0, 1, "ответ", true]
  }'
```

---

## 7. Возможные ошибки и узкие места

### 7.1. Инфраструктура и сеть

* **Kafka недоступна** – воркеры не могут подключиться, в логах постоянные ретраи.

  * Решение: проверить `KAFKA_BOOTSTRAP`, состояние контейнеров Kafka/Zookeeper.
* **Elasticsearch не готов** – ошибки при индексации/поиске.

  * Решение: дождаться инициализации индекса (infra/chunks.json, es-init), проверить логи ES.
* **MinIO не готов или неверные ключи** – ошибки при чтении/записи объектов.

  * Решение: сверить `S3_ENDPOINT`, `S3_ACCESS_KEY`, `S3_SECRET_KEY`, `S3_BUCKET`.

### 7.2. Модели и ресурсы

* **Vectorizer не может загрузить модель** (особенно при `DISABLE_MODEL_DOWNLOAD=1`).

  * Узкое место: пустой кэш моделей.
  * Решение: предварительно скачать модель, смонтировать корректный каталог кэша.
* **LLM работает слишком медленно**:

  * Узкие места: CPU, отсутствие GPU, слишком большая модель, высокие значения `max_tokens`.
  * Решение: использовать более компактную модель, ограничивать длину контекста, настроить температуру и токены.

### 7.3. Качество данных и индексации

* Низкое качество OCR → фрагменты текста плохого качества, ухудшается поиск.
* PDF с нестандартной структурой → пустые страницы или некорректное извлечение текста.
* Неправильный `course_id` при индексировании → материалы «не находятся» по запросам.

### 7.4. Логика LLM и формат ответов

* LLM может вернуть некорректный JSON для квизов:

  * решено fallback-режимом (генерация cloze-вопросов без LLM);
* Роутер (`/assistant`) может в редких случаях выбрать неочевидный режим:

  * на уровне API предусмотрен fallback на RAG или rephrase.

---

## 8. FAQ (часто задаваемые вопросы)

**Вопрос:** Почему `/answer` возвращает пустой или странный ответ?
**Ответ:** Чаще всего:

* материалы не были проиндексированы (пустой индекс `chunks`);
* `course_id` не совпадает с тем, который использовался при индексировании;
* текста в лекциях мало или он плохо извлечён (OCR).

---

**Вопрос:** Можно ли использовать другую модель LLM?
**Ответ:** Да. Необходимо:

* поместить новую GGUF-модель в каталог `models/`;
* изменить команду запуска `generator` в `docker-compose.yml` (имя файла, параметры);
* при необходимости скорректировать промпты и параметры в `services/api`.

---

**Вопрос:** Можно ли использовать другую модель эмбеддингов?
**Ответ:** Да. Нужно:

* изменить `MODEL_NAME` и `MODEL_CACHE` в переменных окружения `vectorizer`;
* убедиться, что размерность эмбеддингов согласуется с настройками Elasticsearch.

---

**Вопрос:** Обязательно ли использовать MinIO и Kafka?
**Ответ:** Для оффлайн-индексирования можно обойтись одним `ingest_pdfs.py` (PDF → ES).
Для полной функциональности (квизы, асинхронная обработка) MinIO и Kafka необходимы.

---

**Вопрос:** Это действительно агентная система?
**Ответ:** Система использует агентный подход:

* есть высокоуровневый агент-ассистент (`/assistant`), который планирует действия (режимы) на основе запроса;
* есть специализированные сервис-агенты (воркеры), взаимодействующие через общую среду (Kafka, ES, MinIO).
  Формально это агентная архитектура поверх микросервисного стека.

---

## 9. Рекомендации и лучшие практики

1. **Чётко разделять курсы по `course_id`**
   Не смешивать материалы разных курсов в одном идентификаторе.

2. **Следить за качеством исходных PDF**
   Чем лучше исходный текст (а не скан), тем лучше поиск, RAG-ответы и квизы.

3. **Использовать осмысленные промпты**
   При необходимости адаптировать системные сообщения LLM под предметную область курса.

4. **Мониторить логи воркеров и LLM**
   Особое внимание уделять:

   * ошибкам подключения к Kafka/MinIO/ES;
   * ошибкам JSON-парсинга ответов LLM (квизы).

5. **Ограничивать длину контекста**
   Большой контекст ведёт к росту затрат и ухудшению качества. Лучше fewer, но релевантных фрагментов.

6. **Документировать свои `course_id`, индексы и пути в MinIO**
   Это облегчает поддержку и сопровождение проекта, особенно при большом количестве курсов и материалов.

---

Документация описывает, что делает код, как он устроен, как его запустить и использовать, а также типовые проблемы и пути их решения.