# Vectorizer service

Сервис **Vectorizer** предоставляет HTTP-интерфейс для вычисления
векторных представлений текста (эмбеддингов) и простой заглушки
перевода. Используется API-сервисом Assistant-Teacher для:

- построения семантического индекса лекций в Elasticsearch;
- поиска по смыслу (kNN-поиск по эмбеддингам);
- вспомогательных операций, где требуется сравнение текстов.

Сервис реализован на **FastAPI** и, как правило, запускается в составе
общего Docker-стека проекта.

---

## 1. Назначение

Основные задачи:

1. Принимать массив строк и возвращать для каждой эмбеддинг фиксированной размерности.
2. Работать в потоковом режиме (обработка батчей без сохранения состояния).
3. Опционально предоставлять заглушечный эндпоинт перевода `source → target`
   (в текущей версии перевод не выполняется, возвращается исходный текст).

Сервис не хранит данные и не зависит от других микросервисов проекта.

---

## 2. Архитектура и зависимости

Сервис основан на библиотеке
[Sentence-Transformers](https://www.sbert.net/) и использует одну из
предобученных моделей, задаваемую переменной окружения `MODEL_NAME`.

Главные компоненты:

- `FastAPI` – HTTP-сервер;
- `sentence-transformers` – загрузка и использование модели;
- файловый кэш моделей – каталог `MODEL_CACHE`, монтируется из хоста
  (например, `./data/hf-cache`).

Модель загружается **лениво** (при первом запросе к `/embed` или `/health`,
который обращается к модели).

---

## 3. Запуск

### 3.1. Через Docker Compose (рекомендуемый вариант)

В составе общего проекта сервис поднимается автоматически:

```bash
docker compose up -d vectorizer
````

По умолчанию сервис доступен по адресу:

```text
http://localhost:8001
```

Стандартно к контейнеру в `docker-compose.yml` подключается:

* том с кэшем моделей `./data/hf-cache:/models`;
* сеть, общая с API и остальными сервисами.

### 3.2. Локальный запуск (для разработки)

1. Установка зависимостей:

```bash
cd services/vectorizer
pip install -r requirements.txt
pip install -r requirements.torch.txt  # при необходимости
```

2. Настройка переменных окружения (см. раздел 4).

3. Запуск приложения:

```bash
uvicorn app.main:app --host 0.0.0.0 --port 8001 --reload
```

---

## 4. Переменные окружения

Все настройки задаются через переменные окружения и читаются в
`app/main.py`.

### 4.1. Основные настройки модели

| Переменная               | Назначение                                             | Значение по умолчанию                    |
| ------------------------ | ------------------------------------------------------ | ---------------------------------------- |
| `MODEL_NAME`             | Имя/путь модели Sentence-Transformers                  | `sentence-transformers/all-MiniLM-L6-v2` |
| `MODEL_CACHE`            | Каталог кэша моделей внутри контейнера                 | `/models`                                |
| `DISABLE_MODEL_DOWNLOAD` | Блокировка загрузки модели из интернета (`1` / `true`) | `0` (загрузка разрешена, если кэш пуст)  |

**Поведение:**

* Если `DISABLE_MODEL_DOWNLOAD=1` и каталог `MODEL_CACHE` пуст, сервис при
  попытке загрузки модели выбросит ошибку
  `RuntimeError("Model cache is empty and downloads are disabled")`.
  Эндпоинт `/embed` в этом случае будет возвращать HTTP-код `503`.
* Если кэш не пуст, модель загружается только из локального каталога.

### 4.2. Перевод

| Переменная          | Назначение                                              | Значение по умолчанию  |
| ------------------- | ------------------------------------------------------- | ---------------------- |
| `TRANSLATE_ENABLED` | Включение эндпоинта `/translate` (`1` / `true` / `yes`) | `0` (перевод отключён) |

При `TRANSLATE_ENABLED=0` эндпоинт `/translate` возвращает HTTP-код `204`
и исходный набор текстов без изменений.

---

## 5. HTTP API

Базовый URL: `http://localhost:8001`.

### 5.1. `GET /health`

Проверка состояния сервиса и (частично) модели.

**Пример запроса:**

```bash
curl http://localhost:8001/health
```

**Ответ (пример):**

```json
{
  "status": "ok",
  "model": "sentence-transformers/all-MiniLM-L6-v2",
  "cache_dir": "/models",
  "cache_ready": true,
  "translate_enabled": false,
  "dim": 384
}
```

**Поля ответа:**

* `status` – строка состояния (`"ok"` при успешной проверке);
* `model` – имя используемой модели;
* `cache_dir` – путь к каталогу кэша внутри контейнера;
* `cache_ready` – флаг, что каталог кэша существует и не пуст;
* `translate_enabled` – флаг доступности эндпоинта `/translate`;
* `dim` – размерность эмбеддингов (если модель успешно загружена);
* `note` – текстовое пояснение, если модель пока не загружена или произошла ошибка.

> Замечание: `/health` не падает, даже если модель ещё не загружена.
> В этом случае поле `dim` отсутствует, а в `note` возвращается текст ошибки.

---

### 5.2. `POST /embed`

Расчёт эмбеддингов для массива строк.

**Тело запроса (`EmbedReq`):**

```json
{
  "texts": [
    "Первая строка",
    "Вторая строка"
  ]
}
```

**Правила:**

* `texts` – обязательное поле, массив строк;
* если массив пуст, возвращается `{ "vectors": [], "embeddings": [] }` без обращения к модели.

**Ответ:**

```json
{
  "vectors": [
    [0.01, 0.02, "..."],
    [0.03, 0.04, "..."]
  ],
  "embeddings": [
    [0.01, 0.02, "..."],
    [0.03, 0.04, "..."]
  ]
}
```

Поля `vectors` и `embeddings` содержат одинаковые данные и
предоставлены для совместимости с существующими клиентами.

**Коды ответов:**

* `200` – успешный расчёт;
* `503` – сервис недоступен (например, модель не может быть загружена из-за
  пустого кэша при `DISABLE_MODEL_DOWNLOAD=1`).

---

### 5.3. `POST /translate`

Простейший эндпоинт перевода. В текущей реализации сам перевод не
выполняется, эндпоинт служит заглушкой и точкой расширения.

**Тело запроса (`TranslateReq`):**

```json
{
  "source": "en",
  "target": "ru",
  "texts": [
    "Some text to translate"
  ]
}
```

Поля:

* `source` – идентификатор исходного языка (строка, не используется в логике);
* `target` – идентификатор целевого языка;
* `texts` – массив строк для перевода.

**Поведение:**

* Если `TRANSLATE_ENABLED = 0`:

  * возвращается HTTP-код `204 No Content` и массив исходных текстов
    (на уровне FastAPI может присутствовать тело `{ "texts": [...] }`,
    клиенту достаточно ориентироваться на код 204 и не ждать перевода).
* Если `TRANSLATE_ENABLED = 1`:

  * возвращается HTTP-код `200` и JSON:

```json
{
  "texts": [
    "Some text to translate"
  ]
}
```

В дальнейшем сюда может быть интегрирован реальный переводчик.

---

## 6. Особенности работы

1. **Ленивая загрузка модели**
   Модель загружается только при первом запросе, который требует её
   использования (`/embed` или `/health`, если запрашивается размерность).
   Это уменьшает время старта контейнера, но увеличивает время первого запроса.

2. **Кэш моделей**
   При корректной настройке тома с кэшем (`MODEL_CACHE`) модель
   скачивается один раз и переиспользуется при последующих запусках контейнера.

3. **Отсутствие состояния**
   Сервис не хранит состояния между запросами. Эмбеддинги пересчитываются
   по запросу, а результаты не кешируются внутри сервиса.

4. **Использование в других сервисах**

   * `services/api` обращается к `/embed` при:

     * построении эмбеддингов для пользовательских запросов (kNN-поиск),
     * оффлайн-индексировании через `ingest_pdfs.py` (если указывают `--vectorizer`).
   * `services/worker/parser_worker` вызывает `/embed` для построения
     эмбеддингов фрагментов лекций при парсинге PDF.

---

## 7. Типичный сценарий использования

1. Запуск стека (включая `vectorizer`) через `docker compose`.

2. Проверка доступности:

   ```bash
   curl http://localhost:8001/health
   ```

3. Расчёт эмбеддингов в приложении:

   ```bash
   curl -X POST http://localhost:8001/embed \
     -H "Content-Type: application/json" \
     -d '{
       "texts": [
         "первый текст",
         "второй текст"
       ]
     }'
   ```

4. Использование полученных векторов для kNN-поиска в Elasticsearch
   или других задач, завязанных на семантическое сравнение текста.

---

Этот документ описывает назначение, настройки и HTTP-интерфейс сервиса
**Vectorizer** в составе проекта Assistant-Teacher и предназначен для
разработчиков, интегрирующих данный сервис в свои компоненты.